# Activation-Attack
Based on CVPR 2019 Paper Feature Space Perturbations Yield More Transferable Adversarial Example
<p>
<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf">
The paper is here</a></p>
 
<p><a href="https://github.com/QwQ2000/Activation-Attack-Pytorch">Code reference from here</a></p>
<p><a href="https://github.com/huyvnphan/PyTorch_CIFAR10">The pretrained model is here<p>
  


<div style="padding:36px">

# <center>特征空间扰动产生更多可转移的对抗样本</center>
<hr>
<font face="黑体">

## 摘要
&emsp;&emsp;最近的许多研究表明，深度学习模型容易受到准潜移默化输入扰动的影响，但实践者无法完全解释这种行为。这项工作描述了一种基于传输的针对深层特征空间表示的黑盒攻击，它还提供了对深层CNN的跨模型类表示的见解。该攻击明确设计用于可转移性，并将L层源图像的特征空间表示推向L层目标图像的表示。该攻击产生高度可转移的目标示例，在目标攻击指标上比竞争获胜的方法高出30%以上。我们还表明，选择L生成示例是重要的，可转移性特征是黑盒模型不可知的，并且表明训练有素的深度模型具有相似的高度抽象表示。
