# Activation-Attack
Based on CVPR 2019 Paper Feature Space Perturbations Yield More Transferable Adversarial Example
<p>
<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf">
The paper is here</a></p>
 
<p><a href="https://github.com/QwQ2000/Activation-Attack-Pytorch">Code reference from here</a></p>
<p><a href="https://github.com/huyvnphan/PyTorch_CIFAR10">The pretrained model is here<p>
  


<div style="padding:36px">

# <center>特征空间扰动产生更多可转移的对抗样本</center>
<hr>
<font face="黑体">

## 摘要
&emsp;&emsp;最近的许多研究表明，深度学习模型容易受到准潜移默化输入扰动的影响，但实践者无法完全解释这种行为。这项工作描述了一种基于传输的针对深层特征空间表示的黑盒攻击，它还提供了对深层CNN的跨模型类表示的见解。该攻击明确设计用于可转移性，并将L层源图像的特征空间表示推向L层目标图像的表示。该攻击产生高度可转移的目标示例，在目标攻击指标上比竞争获胜的方法高出30%以上。我们还表明，选择L生成示例是重要的，可转移性特征是黑盒模型不可知的，并且表明训练有素的深度模型具有相似的高度抽象表示。

## 1 介绍
&emsp;&emsp;许多研究人员已经展示了如何故意利用对抗性攻击来欺骗训练有素的深度学习算法 [23,5，2,20，8]。大多数工作的重点是对模型产生最大的破坏性影响，同时对数据引入最小的扰动。在基于图像的数据的情况下，攻击者通过对图像的不可察觉的扰动来破坏网络的分类能力 [23,5，1 1 6,1 5]。这些攻击更加令人担忧，因为我们作为一个社区对这些基本上无法解释的深层模型内部正在发生的事情没有坚定的理解。但是，**攻击也可能提供一种研究此类模型内部工作原理的方法**。
&emsp;&emsp;对于在CNN上操作的对手来说，更困难的威胁模型之一是黑匣子目标攻击。在这种情况下，对手只能访问输入和输出，而不了解底层权重或体系结构。更重要的是，对手可能会选择甲骨文网络错误分类的目标类别。在这项工作中，我们为deepCNN模型设计了一个针对黑匣子的对抗性攻击。我们利用对抗性可转移性的特性设计攻击，其中对一个模型具有对抗性的示例通常对其他模型具有对抗性[11,18,13]。这种攻击的独特之处在于，它明确干扰了深层模型的特征空间，目的是创建更多可转移的对抗性示例。**这种直觉来自于这样一种观察：训练有素的模型的中间特征是可转移的[26]**。因此，中间特征的扰动也可以转移。
&emsp;&emsp;为了验证这个假设，我们设计了激活攻击 (AA)，如图 1 所示。通过扰动源图像，该算法驱动源图像上白盒模型的 L 层激活，朝向目标图像的该模型的 L 层激活。是特征空间中的扰动会影响模型。我们通过将扰动图像输入黑盒模型来测试扰动特征的可转移性。然后，这些扰动会对黑盒模型层产生无法直接观察到的影响，而黑盒模型层已经了解到了这些相同的特征。
<center><image src="./images/a01.png"><br/><font size=2> 图 1：激活攻击示意图。 鉴于白盒模型 (f_w) 和黑盒模型 (f_b) 最初是正确的，攻击将狗图像的 L 层激活推向飞机的 L 层激活。 受到攻击后，狗的激活与飞机相似，并且扰动图像被归类为飞机到 f_w 和 f_b。</font></center>

&emsp;&emsp;这项工作对对抗性攻击和模型可解释性研究做出了一些贡献。 首先，我们表明构建具有特征空间扰动的对抗性示例会产生可转移的对抗性示例。 此外，特征受到干扰的层对对抗样本的可迁移性有很大影响。 接下来，我们使用规范的 CNN 架构来展示攻击对强大算法的功效，而不是针对简单任务（即 MNIST 分类器）的自定义模型。 对于模型可解释性，我们表明黑盒模型架构不会影响分层可迁移性的特征。 我们还提供了为什么一个特定的层比另一个层产生更多可转移的例子的证据。 最后，这项工作证明了具有根本不同架构的深度学习模型的中间特征表示是相似的。 此外，那些训练有素的模型在特征空间中具有相似的分类方向和相似的决策边界。

## 2 相关工作
&emsp;&emsp;相关工作的一个领域是对抗性攻击研究。许多攻击的目标是对输入图像引入不可察觉的扰动，这对分类器的性能有毁灭性的影响，但对人类识别没有影响。Szegedy等人[23]和Goodfellow等人[5]率先表明，在一个模型上生成的对抗性示例也可能会转移到其他模型。然而，可转移性并不是为之而设计的，而是方法和训练有素的模型的性质的结果。**Papernot等人[18]开发了一种使用储层采样训练替代模型的方法**，以便替代模型生成更多可转移的示例。这项工作显示了一系列机器学习算法的结果，如支持向量机、决策树、DNN，但较少关注现代CNN中的含义。
&emsp;&emsp;**最近，Tram´er et al. [25] 更详细地探索了可转移的对抗性示例**，并研究了为什么通过小型定制 MNIST 模型和非目标攻击会发生可转移性。 具体来说，他们发现模型决策边界在任意方向上是相似的，并且对抗性示例跨越输入的低维子空间。 他们还讨论了特征图在类别均值方向上的扰动，但在 CNN 上的结果很差且没有定论。最后，作者提供的证据表明，在某些情况下，针对同一任务的训练有素的模型（两者都表现出易受简单攻击的弱点）可能会传递敌对的例子。我们的工作不同之处在于攻击是有针对性的，我们针对非平凡问题在规范 CNN 上对分层可迁移性进行了广泛的实验，并且我们进一步分析了示例迁移的原因。
&emsp;&emsp;**Sabour等人[21]也是最早明确描述特征空间中仅白盒攻击的人之一。** 它们使用了一种昂贵的L-BFGS摄动方法，并表明不同类别的示例可以在特征空间中彼此非常接近，但仍然保持其原始图像结构。然而，作者简要地提到，受干扰的例子**不能很好地作为目标攻击转移到黑盒模型中，而且攻击成本相当高。**
&emsp;&emsp;2017年，有一场对抗性攻击和防御的竞赛[11]，其中攻击者的目标是通过非目标攻击和目标攻击击败黑盒模型。大多数性能最好的攻击[4,11,13,14]都是从[13,24]中介绍的一组白盒模型中生成和传输对抗性示例。获奖者还使用了动量梯度去噪技术[4]，该技术被纳入了这项工作中。大多数攻击都是对现有攻击的直接扩展，以进行集成操作。我们在这里不使用集成攻击，而是将重点放在一种新的微扰技术上，该技术可能会随着系综而扩展，作为未来的工作。
&emsp;&emsp;**相关工作的另一个领域是模型的可解释性**，特别是深度模型的共享特征表示。Yosinski等人[26]表明，使用类似分布数据训练的模型的深层特征表示是可转移的。他们还测量了从广义特征到高度类特定（即专门化）特征的过渡，作为层深度的函数。这是迁移学习的一项开创性工作。虽然[26]不是对抗性攻击，但它确实提供了特征空间攻击可能转移的直觉。

## 3 可转移性指标
&emsp;&emsp;在这项工作中，我们将通过四个指标来定义成功：错误率（$error$）、非目标传输率（$uTR$）、目标成功率（$tSuc$）、目标传输率（$tTR$）。 重要的是，我们假设所有示例都被白盒和黑盒模型正确分类，并且 $ε = 0$ 的攻击强度意味着没有攻击。 我们将白盒模型表示为函数 $f_w$，将黑盒模型表示为 $f_b$，两者都输出分类预测。我们定义我们的原始数据集
$$D_{\text {orig }}=\left\{\left(x^{(1)}, y_{\text {true }}^{(1)}\right), \ldots,\left(x^{(N)}, y_{\text {true }}^{(N)}\right)\right\}$$作为一组$N$个数据/标签对，其中$f_{b}\left(x^{(i)}\right)=f_{w}\left(x^{(i)}\right)=y_{\text {true }}^{(i)}$
对于每次攻击，我们都会创建一个对抗性数据集$$D_{a d v}=\left\{\left(x_{a d v}^{(1)}, y_{\text {target }}^{(1)}, y_{\text {true }}^{(1)}\right), \ldots,\left(x_{a d v}^{(N)}, y_{\text {target }}^{(N)}, y_{\text {true }}^{(N)}\right)\right\}$$其中$D_{orig}$ 中的每个数据都受到 $f_w$ 上的针对性攻击方法的干扰，使其成为对抗性示例。
&emsp;&emsp;攻击的错误率（$error$）或愚弄率是使用$ f_w $生成的对抗样本被$f_b$错误分类的百分比。换句话说，错误是$f_b(x_{adv})\ne y_{true}$的$D_{adv}$中的示例的百分比。越大的错误表示越有效的攻击。
&emsp;&emsp;非目标传输率 ($uTR$) 是欺骗白盒模型的特定示例也欺骗黑盒模型的速率。请注意，在这种情况下愚弄意味着预测不等于真实标签。为了测量$uTR$，我们定义了$D_{uTR}⊆D_{adv}$，它只包含被$f_w$错误分类的$D_{adv}$元素。因此
$$u T R=\frac{1}{\left|D_{u T R}\right|} \sum_{\substack{\left(x_{a d v}, y_{t r u e}\right) \\ \in D_{u T R}}} \mathbb{1}\left[\left(f_{b}\left(x_{a d v}\right)\right) \neq y_{\text {true }}\right)]\quad(1)$$

&emsp;&emsp;其中$1$是指示函数，如果条件为真，则为 1，否则为 0。该指标直观地编码了攻击者白盒模型的成功无目标对抗样本也与黑盒模型对抗的可能性。
&emsp;&emsp;由于所考虑的攻击都是有针对性的，因此我们测量了目标成功率 ($tSuc$)。  $tSuc$ 是使用$f_w$ 生成的对抗样本被$ f_b $分类为目标标签的比率。 换句话说，$tSuc $是在 $D_{adv}$ 中 $f_b(x_{adv}) = y_{target} $的示例的百分比。  $tSuc $越大，为黑盒模型生成目标示例的攻击就越有效。
&emsp;&emsp;最后一个指标是目标传输率 ($tTR$)，它衡量在白盒模型上测量的成功的目标对抗样本也是在黑盒模型上成功的目标样本的比率。为此，我们定义 $D_{tTR} ⊆ D_{adv}$（也是 $D_{uTR}$ 的一个子集），其中包含被 $f_w$ 错误分类为指定目标标签的所有 $D_{adv} $元素。正式地，
$$t T R=\frac{1}{\left|D_{t T R}\right|} \sum_{\substack{\left(x_{\text {adv }}, y_{\text {target }}\right) \\ \in D_{\text {tTR }}}} \mathbb{1}\left[\left(f_{b}\left(x_{a d v}\right)\right)=y_{\text {target }}\right)]\quad(2)$$

&emsp;&emsp;$tTR$ 编码了在白盒模型上观察到的成功的目标示例将成为黑盒模型的成功目标示例的可能性。当前有针对性的攻击文献通常测量错误和 $tSuc$。我们引入$ uTR $和 $tTR$ 作为新的攻击指标，如果攻击者希望在有限的尝试次数中最大化其成功机会，这将非常有用。另请注意，即使攻击是有针对性的，我们也会测量非目标统计数据，因为它们仍然与攻击的威力相关。

## 4 激活攻击方法
&emsp;&emsp;激活攻击 (AA) 是一种针对可转移性的黑盒针对性攻击。我们可以将激活攻击方法视为分为两个部分，它们模仿了许多深度学习攻击实践。首先，我们指定要优化的损失函数。然后，我们建立了攻击算法和摄动方法，根据损失调整数据。

## 4.1 损失函数

&emsp;&emsp;AA损失函数定义为L层的矢量化源图像激活和矢量化目标图像激活之间的欧氏距离。假设$f_L$是$f_w$（白盒模型）的截断版本，将图像作为输入并输出$L$层的激活。因此，$A_{s}^{L}=f_{L}\left(I_{s}\right)$是源图像（$I_s$）激活之后的图像，$A_{t}^{L}=f_{L}\left(I_{t}\right)$是目标图像（$I_t$）激活在$L$之后的图像，两幅图像之间的损失函数$J_{AA}$为，$$J_{A A}\left(I_{t}, I_{s}\right)=\left\|f_{L}\left(I_{t}\right)-f_{L}\left(I_{s}\right)\right\|_{2}=\left\|A_{t}^{L}-A_{s}^{L}\right\|_{2}\quad (3)$$

&emsp;&emsp;AA损失函数背后的直觉是使源图像在特征空间中更接近目标类的图像。这种损失的影响/假设是三重的。首先，深度特征空间表示的调整对分类结果有相当大的影响。由于我们没有明确优化分类损失，我们依赖于特征空间扰动的副产品，即显着的分类破坏。因为特征空间表示是如此之大且无法解释，所以这种情况并不是很明显。此外，由于特征空间的大小（即参数数量），我们必须假设受约束的图像域扰动能够将原始样本移动到足够接近目标样本（在特征空间中），就像在目标类的区域内。
&emsp;&emsp;使用此损失函数做出的第二个主要假设是：由于深度模型的中间层特征已被证明是可转移的 [26]，因此特征空间中的显式攻击将产生可转移的对抗样本。 由于现代深度模型难以理解，因此无法准确测量在模型的每一层中捕获和学习了哪些特征。 因此，没有办法明确知道两个模型是否学习了相似的特征集，尤其是在深度和高度抽象的层中。 这种攻击主要假设不同深度模型的深层已经学习了相似的特征，因此在一个模型中扰乱这些高度抽象的特征将扰乱另一个模型中的相同特征。 这是一个合理的假设，因为在可迁移性攻击中，我们使用与黑盒模型的训练数据相同分布的数据来训练白盒模型。 因此，我们希望这些模型已经学习了相似的分层特征集，以便正确地对数据类别进行建模。 由于每个模型架构在复杂性、层数和整体架构上都不同，因此找到特征最易迁移的层需要进行实验。
&emsp;&emsp;第三个主要假设是关于学习的决策边界和特征空间中的类方向。具体来说，我们假设使用来自相同分布的数据训练的两个不同模型学习相似的决策边界和类别方向。这与针对性攻击特别相关，因为为了使转移的目标示例成功，特征空间中该目标类的区域必须具有相同的方向 w.r.t。源图像。换句话说，如果我们从白盒模型中源样本的特征空间表示开始，向目标样本的方向移动，这个假设表明移动方向在黑盒模型中是相同（或至少相似）的.

### 4.3 攻击算法

&emsp;&emsp;微扰机制与$L_∞$ 带动量的约束迭代梯度符号攻击（TMIFGSM）[4]。我们的攻击算法使用动量项的符号对源图像进行迭代扰动，其中动量被计算为梯度的加权累积。这里的区别在于，梯度不是根据分类损失计算的。相反，它们是根据（3）计算的。此外，梯度从$L$层开始向后流动。因此，动量计算如下：
$$m_{k+1}=m_{k}+\frac{\nabla_{I_{k}} J_{A A}\left(I_{t}, I_{k}\right)}{\left\|\nabla_{I_{k}} J_{A A}\left(I_{t}, I_{k}\right)\right\|_{1}}\quad (4)$$

&emsp;&emsp;其中$m_0=0$，$I_k$是迭代$k$时的扰动源图像。注意，$I_0=0$。这个目标的摄动方法$L_\infty$约束激活攻击是
$$I_{k+1}=\operatorname{Clip}\left(I_{k}-\alpha * \operatorname{sign}\left(m_{k+1}\right), 0,1\right) .\quad(5)$$

&emsp;&emsp;注意，扰动图像总是被裁剪到 $[0, 1] $范围内，以保持原始图像的分布。  公式(5) 的内在含义是我们在最小化我们的$ JAA $损失的方向上稍微调整图像的每个像素。 动量项用于直观地对梯度方向进行去噪或平滑，并在 [4] 中描述为梯度方向上速度向量的累积。 另外，请记住，我们扰乱图像的明确意图是改变$ L $层的特征空间表示。因此，对分类的任何影响都是隐含的，因为我们没有专门考虑分类损失。
&emsp;&emsp;该算法还需要设置一些超参数。由于这是一个迭代算法，我们必须选择要扰动的迭代次数$K$、总扰动量$\epsilon$和每次迭代的扰动量$α$。在所有测试中，我们设置$K=10$，改变$\epsilon$，并设置$α=\epsilon/K$。

## 5 实验设置

&emsp;&emsp;如在 [25] 中观察到的，示例倾向于在实现源任务低误差的模型之间进行转换。因此，我们选择 CIFAR-10 [9] 作为我们的主要测试数据集，因为它并不重要，但最先进的模型可以实现小于$ 10\% $的测试错误 [12]。对于主要实验，我们选择并训练了三种具有不同设计复杂性的规范 CNN 模型架构，它们能够在 CIFAR-10 上实现低错误。我们使用 ResNet-50 [6] 实现了 $6.62\%$ 的 top-1 测试错误，DenseNet-121 [7] 实现了 $4.72\%$ 的错误，以及 VGG19bn [22] 测试了 $6.48\%$ 的错误。所有模型都使用 [12] 中的代码在 PyTorch [19] 中进行了训练。为了完整起见，我们还将一些实验扩展到 ImageNet [3] 训练的模型。我们使用来自 PyTorchs Torchvision 模型的预训练 DenseNet-121 和 ResNet-50。这些模型对源任务产生显着更高的错误，其中 DenseNet-121 有 $25.35%$ 的 top-1 错误，而 ResNet-50 有 $23.85\%$ 的错误。
&emsp;&emsp;对于 CIFAR-10 测试，我们测量了整个 10k 测试集的四个主要指标。 首先，我们使用 DenseNet-121 作为白盒模型，并分别评估对 VGG190 亿和 ResNet-50 黑盒模型的可迁移性。 接下来，我们使用 VGG190 亿作为白盒模型，并分别评估对 DenseNet-121 和 ResNet-50 黑盒模型的可迁移性。 这使我们能够一起查看白盒模型和黑盒模型的趋势。 对于 ILSVRC2012 测试，我们在全部 50k 测试集的 15k 随机抽样子集上运行一个主要实验。 在这里，我们测试了 DenseNet-121 白盒模型和 ResNet-50 黑盒模型。
&emsp;&emsp;关于设置的注意事项是关于 AA 中目标图像的选择。对于每个数据集，我们保留了每个类的示例库，这些示例都是从测试拆分中随机抽取的。对于 CIFAR10，我们保留每个类的 100 个示例，对于 ImageNet，每个类保留 20 个示例。对于给定的源图像，我们随机选择一个目标类，然后从库中选择目标图像作为离源图像激活最远层 L 激活（由欧几里德距离测量）的目标图像。另外，请注意，以下实验中的层深度是相对的，即第 2 层是指比第 10 层更靠近输入的层。此外，在所有测试中，测试的最深层是产生输出类 logits 的最终 FC 层，并且采样层均匀分布在模型中。为每个模型解码层的表格在补充材料中。

## 6 实验结果

&emsp;&emsp;为了了解 AA 攻击是否可行，我们将首先进行测试并收集经验结果，然后分析我们的发现。有两个参数的主轴要测试：epsilon 和 depth。 Epsilon 测试分别处理来自每一层的攻击，并测量 epsilon 对可迁移性的影响。沿深度轴的测试涉及固定攻击强度和测量攻击性能，作为我们从哪个层生成 AA 示例的函数。对于这些测试，我们使用三个基线。迭代目标类方法 (ITCM) [10] 是基本迭代方法的目标变体，代表一种简单方法。从 [14] 随机开始的目标投影梯度下降 (TPGD) 攻击代表了一种更复杂的方法，该方法通常在其他基线之间执行。目标动量迭代快速梯度符号法 (TMIFGSM) [4] 代表了我们最复杂的方法，该方法赢得了 2017 年 NIPS 攻击和防御竞赛 [11] 的目标和非目标攻击。对白盒模型的每次攻击的性能显示在补充文件中。

### 6.1 Epsilon 结果
&emsp;&emsp;epsilon 测试是两个具有共同目标的实验的结果，以欺骗 ResNet-50 分类器。首先，我们测量将示例从 DenseNet-121 转移到 ResNet-50（DN121 → RN50）时的四个可转移性指标，然后运行 ​​VGG190bn → RN50 测试。对于两者，$\epsilon$被扫描为$ [0.0, 0.01, 0.03, 0.05, 0.07]$，其中 $\epsilon = 0$ 表示没有攻击。结果如图2所示。表示的AA攻击来自最好的层。
<center><image src="./images/a02.png"><br/><font size=2> 图2:CIFAR-10攻击从不同的白盒模型转移到ResNet-50黑盒模型的可转移性与epsilon结果。DN和VGG代表DenseNet121和VGG19bn白盒模型。</font></center>

&emsp;&emsp;我们看到的第一个趋势是，对于所有指标的所有攻击，随着的增加，攻击强度也会增加。我们还看到DenseNet-121 AA（DN-AA）是最强大的攻击，而ITCM攻击最不有效，TMIFGSM介于两者之间。在$\epsilon=0.07$时，DNAA在黑盒模型上实现了$91.42\%$的随机精度，比最佳基线提高了$7.4\%$。它在uTR、tSuc和tTR方面的性能分别比DN-TMIFGSM基线高出$7.2\%$, $32.6\%$和$32.5\%$。此外，DN-AA和VGG-AA在$tSuc$和$tTR$方面都高于所有基线，表明这两种基于激活的攻击都比基线更好。DN-AA优于VGG-AA的事实表明白盒模型架构确实会影响AA性能。然而，更复杂的DN121模型更适合RN50黑盒模型并不奇怪，因为与相对较浅的VGG相比，这两个模型都相当深。

### 6.2 Deepth 结果

&emsp;&emsp;我们的假设最重要的结果是深度结果。 在这里，我们固定 $\epsilon = 0.07$，因为这是最强大的攻击能够在黑盒模型上实现随机准确度的地方，尽管我们的结论适用于所有测试的 epsilon。 然后，我们在模型的不同深度测试 AA，在每个模型上运行一个完整的测试步骤。 图 3 显示了深度扫描测试的结果，表 1 显示了最强大的层 AA 攻击与基线的数值结果。 在表 1 中，DN 和 VGG 是 CIFAR-10 训练的白盒模型，DNIN 是 ImageNet 训练的。
&emsp;&emsp;图 3 的前两行是来自 DN121 白盒模型的转移，后两行来自 VGG190bn白盒模型。我们看到逐层可迁移性特征不依赖于黑盒模型。意思是，趋势线的形状不会随着黑盒模型而改变。这对攻击的可行性至关重要，因为这意味着攻击者可以使用自己的黑盒模型找到最佳传输层，然后使用该知识攻击真正的目标黑盒模型。这限制了对目标模型的查询量。此外，两个 DN121 白盒测试在更深层都显示出强大的迁移能力，两个 VGG190bn测试在中间层都显示出强大的迁移能力。我们还看到，来自某些层的 AA 产生的攻击威力较小，这表明 AA 层的选择至关重要。
<center><image src="./images/a03.png"><br/><font size=2> 图 3：多种传输场景的误差、uTR、tSuc 和 tTR 率与深度的关系。前两行是来自 DN121 白盒模型的传输，下两行是来自 VGG190 亿白盒模型的传输。数据集：CIFAR-10。</font></center>
&emsp;&emsp;
<center><font size=2> 表1: 数值传递结果 (RN50黑盒)</font><br><image src="./images/t01.png"><br/></center>

&emsp;&emsp;如果我们选择单个最佳层进行攻击，我们可以将数值性能与最佳基线进行比较，在所有情况下都是 TMIFGSM（见表 1）。 对于 DN121 模型，最佳 AA 来自$ L = 21$，而对于 VGG190bn，最佳 AA 来自$ L = 6$。当转移到 RN50 时，$DN121_{L=21}$攻击的误差优于最佳基线$ 7.4\%$，$uTR$ 为$ 7.2%$，  $tSuc$ 为 $32.6\%$，$tTR$ 为 $32.5\%$。 同样，当从 VGG190bn转移到 RN50 时，$VGG19_{L=6} $攻击在误差、$uTR$、$tSuc$ 和 $tTR$ 上分别优于最佳基线$ 4.6\%$、$4.2\%$、$17.9\%$、$14.8\%$。 注意，对于 DN121 白盒，错误率和非目标传输率均约为 $91.5\%$，目标成功率和传输率均约为$ 75.2\%$。 如此高的数字表明这些深度和复杂的模型已经学习了相似的特征集和相似的决策边界结构。
&emsp;&emsp;最后，我们可以直接比较 AA 攻击的性能。转移到 RN50 时，$DN121_{L=21}$ 在误差、$uTR$、$tSuc$ 和 $tTR$ 上分别比 $VGG19_{L=6 }$高 $10.6\%$、$10.5\%$、$19.5\%$、$19.4\%$。因此，我们再次发现选择要从中迁移的白盒模型很重要。


### 6.3 分析
&emsp;&emsp;下一个自然要问的问题是为什么有些层比其他层好，特别是为什么$DN121_{L=21}$和$VGG19_{L=6 }$？ 鉴于图 3 中的可迁移性趋势不会随黑盒模型而改变，为了回答这个问题，我们只考虑白盒模型和扰动数据的特征。 第一个实验是测量特征空间中类表示的可分离性。 直观地说，我们期望具有良好可迁移性特征的 AA 来自具有良好分离的类表示的层。图 4 显示了同一类的示例（类内）和不同类的示例（类间）之间的平均角距离，跨越层深度。对于分离良好的层，我们预计类间距离较高，类内距离较低，而两者之间的差异较大。图 4 显示，对于 DN121，类在较晚的层中分离得很好，而在 VGG190 亿的较早。因此，从（$DN121_{L=21}$ 和 $VGG19_{L=6 }$）传输的最佳层具有良好分离的类表示。但是，差异最大的层不一定是最可转移的。
<center><image src="./images/a04.png"><br/><font size=2>图4:DN121和VGG19bn CIFAR-10训练模型的同一类（类内）和不同类（类间）示例的特征图之间的平均角距离。</font></center>

&emsp;&emsp;另一个实验是纯粹看数据和测量确定每一层的原始样本和扰动样本之间的平均距离。我们在图像域和二维中测量欧几里得距离。在这里，我们投影到干净数据的前两个主成分方向，以沿着最大方差方向测量攻击的影响。本能地，我们可能期望具有更好的可传递性特征的层会产生与原始数据更远的对抗性示例，因为它们可能更有可能越过决策边界。图5显示了在DN121和VGG19白盒模型上针对具有一定数量特征可分离性的层的这些实验结果。对于DN121，层16至21产生扰动示例，这些扰动示例在距离原始数据的两个维度上更远，但在图像域中更近。即使$L = 21$不产生最远的示例，二维测量的趋势类似于图3中的DN121可传递性趋势，其中层16-21具有最佳的可传递性特征。类似地，对于VGG19测试，$L = 6$产生的示例在二维上离原始数据最远，而在图像域中，这些扰动的示例往往更接近原始数据。VGG19在两个维度上的趋势也模仿了图3的趋势。
<center><image src="./images/a05.png"><br/><font size=2>图 5：原始图像和 AA 扰动图像之间的 L2 距离，在图像域（红色）和投影到干净数据的前两个主分量方向（蓝色）时测量。从 DN121 和 VGG190 亿 CIFAR-10 训练模型生成的示例。</font></center>

&emsp;&emsp;从这个分析中，我们观察到一些趋势表明产生可转移对抗样本的层。 首先，这些层在特征空间中具有良好分离的类表示。 其次，这些层生成的示例更接近图像域中的原始数据，但在前两个主成分方向上更远。 不幸的是，这些结果都不是绝对结论性的。 因此，我们将其留给未来的工作来更彻底地探索分层可迁移性特征的原因。 然而，通过这些分析技术，我们现在可以对使用不同数据和模型时哪些层最可转移做出明智的预测，避免了昂贵的扫描层步骤。 有关 SVHN [17] 训练模型的分析优先实验，请参阅补充材料。

### 6.4 在ImageNet上的结果

&emsp;&emsp;现在，我们将实验扩展到ImageNet训练的分类器。在这里，我们测量DN121 → RN50攻击的可传递性，并对结果进行类似的分析。图6显示了固定的$\epsilon=0.07$攻击的可传递性结果，表1的$DN_{IN}$行显示了数值结果。有趣的是，我们在这里看到了几种不同的趋势。首先，与CIFAR-10测试不同，早期和中间层DN121 AAs更具传递性。
<center><image src="./images/a06.png"><br/><font size=2>图6: ImageNet训练的DN121 → RN50传输场景的错误、uTR、tSuc和tTR速率与深度的关系。</font></center>


&emsp;&emsp;接下来，所有攻击的目标转移统计量 $tSuc $ 和  $tTR $ 都较低。 考虑  $DN121_{L=7} $，这可以说是最好的 AA 层。 与最佳基线相比， $DN121_{L=7}$ AA 的误差、 $uTR $、 $tSuc $ 和  $tTR $ 分别提高了  $32.8\% $、 $34.0\% $、 $0.3\% $、 $6.3\% $。  CIFAR-10 测试中  $tSuc $ 和  $tTR  $的下降说明这些不太准确的模型的决策边界存在差异。
&emsp;&emsp;AA 在欺骗率和错误分类的可能性（即 uTR）方面比基线有很大的优势。这表明从示例到附近决策边界的方向在某些层中是相似的。但是，模型之间的特征空间中类的整体布局和方向并不相同。这凸显了基于单一模型传输的针对性攻击的根本弱点。如果模型没有针对源任务进行良好的训练，则特征空间中的决策边界不是高度相似，因此很难找到向目标类区域移动的方向。**一个有趣的未来工作是从一个集成中生成 AA 示例，这可能会找到一个平均移动方向。**
&emsp;&emsp;图 7（上）显示了原始（干净）和扰动示例之间的层可分离性和距离分析。毫不奇怪，从 tSuc 和 tTR 结果来看，DN121 ImageNet 模型中的所有层似乎都没有在特征空间中具有良好分离的类。因此，我们期望将我们的源特征推向单个目标示例的特征并不一定意味着我们正在向特征空间中该目标类的大区域进行驱动。未来的一项潜在工作是推动目标类示例的质心，这可能会提高目标性能并提高 AA 的 tSuc 率。
<center><image src="./images/a07.png"><br/><font size=2>图7: 对ImageNet训练的DN121分层特征相似度 (顶部) 以及从该白盒模型 (底部) 使用激活攻击生成的原始示例和对抗示例之间的距离的分析。</font></center>
&emsp;&emsp;当我们观察具有一定程度的可分离性的层时，我们还会在图7 (底部) 中看到变化趋势。与层16至21产生进一步的二维实例不同，较早的层 (10至15) 产生进一步的实例。这类似于图6的发现，其中较早的层比较晚的层更好地转移。这进一步证明，更多的可转移的AA层产生了扰动的示例，这些示例沿着主成分方向远离原稿，但更靠近图像域中的原稿。

# 7 结论

&emsp;&emsp;这项工作描述了使用特征空间扰动的对抗性攻击，它还提供了对深度学习模型如何做出决策的见解。我们证明，对于训练有素的模型，特征空间扰动是高度可转移的，并且扰动转移的层对攻击有效性有很大影响。此外，白盒模型的分层可传递性特征与黑盒模型无关。通过分析，我们发现最适合攻击的层具有很好的分离的类表示，并产生沿主成分方向受到更多干扰的示例。对于可解释性，我们通过显示一个模型的扰动特征也扰动其他模型的那些特征，来表明不同体系结构的深层cnn学习数据的相似层次表示。


</div>
